ğŸ§ AI Voice Detector â€” Training Pipeline

This repository contains the training pipeline for an AI vs Human speech classifier built using:

Wav2Vec2 XLS-R (300M) for speech representation learning

XGBoost for classification

Large-scale multilingual speech datasets

The model learns deep speech embeddings from raw audio and classifies whether a voice is Human or AI-Generated.

ğŸ§  Training Architecture
Audio File
   â†“
Librosa Preprocessing
   â†“
2-sec Chunking
   â†“
Wav2Vec2 Middle Layer Embeddings
   â†“
Mean Pooling
   â†“
XGBoost Classifier
   â†“
Human / AI Prediction

âš™ï¸ Key Design Choices
âœ… Middle Layer Features

Instead of the final transformer layer, we extract:

hidden_states[8]


This captures:

Prosody

Temporal structure

Spectral consistency


âœ… Chunk-Level Processing

Audio is split into 2-second chunks:

Stabilizes embeddings

Reduces noise sensitivity

Improves generalization across datasets

âœ… Embedding Caching

Embeddings are extracted once and saved as .npy files to avoid recomputing expensive Wav2Vec2 forward passes.

embeddings/
 â”œâ”€â”€ human/
 â””â”€â”€ ai/

ğŸ“‚ Project Structure
project/
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ train/
â”‚       â”œâ”€â”€ human/
â”‚       â””â”€â”€ ai/
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ human/
â”‚   â””â”€â”€ ai/
â”‚
â”œâ”€â”€ training_script.py
â””â”€â”€ ai_voice_detector_xgb_v2.pkl

ğŸ› ï¸ Installation
1ï¸âƒ£ Create Virtual Environment
python -m venv venv
source venv/bin/activate

2ï¸âƒ£ Install Dependencies
pip install torch transformers librosa soundfile xgboost scikit-learn tqdm joblib

ğŸš€ Training Steps
Step 1 â€” Load Wav2Vec2

The pipeline loads:

facebook/wav2vec2-xls-r-300m


with:

output_hidden_states=True

Step 2 â€” Extract Embeddings

Run:

process_folder("dataset/train/human", HUMAN_EMB)
process_folder("dataset/train/ai", AI_EMB)


This:

Loads audio

Trims silence

Chunks audio

Extracts embeddings

Saves .npy cache

âš ï¸ This step is GPU-heavy and runs only once.

Step 3 â€” Train Model

Embeddings are loaded and split:

train_test_split(... stratify=y)


XGBoost configuration:

n_estimators=400
max_depth=8
learning_rate=0.05
subsample=0.8
colsample_bytree=0.8
tree_method="hist"

Step 4 â€” Evaluation

Outputs:

Classification report

Confusion matrix

Step 5 â€” Save Model
ai_voice_detector_xgb_v2.pkl


This model is later used in the FastAPI inference server.

ğŸ§ª Inference Example
print(predict_file("test/humanhg.wav"))


Output:

{
  "prediction": "Human",
  "confidence": 0.93,
  "explanation": "Speech characteristics match natural human patterns."
}

ğŸ“Š Dataset Composition (Example)

The training data combines:

Human speech (CommonVoice, OpenSLR, ASVSpoof bonafide)

AI speech (EdgeTTS, ElevenLabs, Respeecher, ASVSpoof spoof)

Designed to improve detection of:

Multilingual speech

Low-quality compressed audio

âš¡ Hardware Requirements

Recommended:

GPU: NVIDIA T4 / A10 / RTX 30xx

RAM: 8GB+

Storage: 20GB+

Embedding extraction is the most compute-intensive step.

ğŸ” Notes

Silence trimming is applied (top_db=25)

Training uses cached embeddings for speed

ğŸ“œ License

MIT License
